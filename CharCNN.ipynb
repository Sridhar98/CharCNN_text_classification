{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CharCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx6hQlsE12b6",
        "colab_type": "text"
      },
      "source": [
        "## **Mount Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeaQAPO-s-az",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "78f05299-942f-472f-d490-035fc224d85f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxpSsgSt1wxA",
        "colab_type": "text"
      },
      "source": [
        "## **Change directory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-ohhxRAtRnH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7134fe76-3813-416c-b5b6-ee20f4cf24f4"
      },
      "source": [
        "cd /gdrive/My Drive/IIITH/CharCNN"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/IIITH/CharCNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC7THYDwYx86",
        "colab_type": "text"
      },
      "source": [
        "## **Utility functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8MKiZNrXEfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_fc_input_size(max_seq_len):\n",
        "        \"\"\"\n",
        "        Returns the value of input_features parameter for the first Linear layer\n",
        "        @params max_seq_len (int): Maximum number of characters considered for\n",
        "        input\n",
        "        @returns x (int): The input_features parameter for first Linear layer\n",
        "        \"\"\"\n",
        "        x = conv_output(max_seq_len,7,0,1)\n",
        "        x = conv_output(x,3,0,3)\n",
        "        x = conv_output(x,7,0,1)\n",
        "        x = conv_output(x,3,0,3)\n",
        "        x = conv_output(x,3,0,1)\n",
        "        x = conv_output(x,3,0,1)\n",
        "        x = conv_output(x,3,0,1)\n",
        "        x = conv_output(x,3,0,1)\n",
        "        x = conv_output(x,3,0,3)\n",
        "        return x\n",
        "\n",
        "def conv_output(input_size,kernel_size,padding_size,stride):\n",
        "    \"\"\"\n",
        "    Returns the output sequence length after a  1d convolution/max pooling\n",
        "    operation according to the formula\n",
        "    output_size = floor(input_size - 2*padding_size - kernel_size)/stride+1\n",
        "    @params input_size (int): Length of input sequence\n",
        "    @params kernel_size (int): number of time steps that the kernel\n",
        "    convolves over\n",
        "    @params padding_size (int): number of pixels used to pad the input on\n",
        "    one size of the input matrix\n",
        "    @params stride (int): Kernel stride\n",
        "    @returns Output sequence length\n",
        "    \"\"\"\n",
        "    return (input_size-2*padding_size-kernel_size)//stride + 1\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZJ-Jdqb1tDJ",
        "colab_type": "text"
      },
      "source": [
        "## **Data Loader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02vgB8NbtXjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import csv\n",
        "import json\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import sys\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "\n",
        "class AGNEWS_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Defines the AG News dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,csv_path,alphabet_path,max_seq_len):\n",
        "        \"\"\"\n",
        "        Initializes the dataset\n",
        "        @params csv_path (str): Path to csv file that contains train/test data\n",
        "        @params alphabet_path (str): Path to json file that contains the\n",
        "        characters considered\n",
        "        @params max_seq_len (int): Maximum number of characters considered]\n",
        "        for input\n",
        "        \"\"\"\n",
        "        self.max_seq_len = max_seq_len\n",
        "        with open(alphabet_path) as f:\n",
        "            self.alphabet = json.load(f)\n",
        "        with open(csv_path) as f:\n",
        "            self.data = csv.reader(f,delimiter=',')\n",
        "            self.data = list(self.data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        \"\"\"\n",
        "        Returns a text, class pair\n",
        "        @params idx (int): Index into the dataset\n",
        "        @returns (self.seq,self.cls) tuple(torch.Tensor,int): Returns a tensor\n",
        "        of shape (num_characters,max_seq_len) representing the input text and an\n",
        "        integer representing the class index\n",
        "        \"\"\"\n",
        "        self.cls = int(self.data[idx][0])\n",
        "        self.seq = torch.zeros(len(self.alphabet),self.max_seq_len)\n",
        "        seq_len = 0\n",
        "        sequence = \"\".join(self.data[idx][1:])\n",
        "        sequence = sequence[::-1]\n",
        "        for char in sequence:\n",
        "            if seq_len > self.max_seq_len:\n",
        "                break\n",
        "            try:\n",
        "                self.seq[self.alphabet.index(char)][seq_len] = 1\n",
        "            except:\n",
        "                pass\n",
        "            seq_len += 1\n",
        "        return self.seq,self.cls\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the dataset length\n",
        "        \"\"\"\n",
        "        return len(list(self.data))\n",
        "\n",
        "def one_hot(data,alphabet):\n",
        "    \"\"\"\n",
        "    Converts a character to its one-hot vector representation\n",
        "    @params data (char): The character that is input to the CNN\n",
        "    @params alphabet (list): The list of characters considered.\n",
        "    NOTE: Characters outside the alphabet are considered to be a zero vector\n",
        "    @returns t (torch.Tensor): Tensor of shape (len(alphabet))\n",
        "    \"\"\"\n",
        "    t = torch.zeros(len(alphabet))\n",
        "    try:\n",
        "        t[alphabet.index(data)] = 1\n",
        "    except:\n",
        "        return t\n",
        "    return t\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrWHrwLq1m_Y",
        "colab_type": "text"
      },
      "source": [
        "## **Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDPJUfZ7Dc_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class charCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines the character level CNN architecture\n",
        "    \"\"\"\n",
        "    def __init__(self,num_features,conv_channel_size,fc_size,num_class,max_seq_len,mean,std):\n",
        "        \"\"\"\n",
        "        Initializes the model\n",
        "        @params num_features (int): The number of features (the number of\n",
        "         characters) considered\n",
        "        @params conv_channel_size (int): Number of 1D Convolutional kernels used\n",
        "        @params fc_size (int): Number of units in the fully-connected layers\n",
        "        @num_class (int): Number of classes in the dataset\n",
        "        @returns object of this class when implicitly called\n",
        "        \"\"\"\n",
        "        super(charCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=num_features,out_channels=conv_channel_size,kernel_size=7,stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3,stride=3)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=conv_channel_size,out_channels=conv_channel_size,kernel_size=7,stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3,stride=3)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=conv_channel_size,out_channels=conv_channel_size,kernel_size=3,stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=conv_channel_size,out_channels=conv_channel_size,kernel_size=3,stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=conv_channel_size,out_channels=conv_channel_size,kernel_size=3,stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=conv_channel_size,out_channels=conv_channel_size,kernel_size=3,stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3,stride=3)\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(in_features=get_fc_input_size(max_seq_len)*conv_channel_size,out_features=fc_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(in_features=fc_size,out_features=fc_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.fc3 = nn.Linear(in_features=fc_size,out_features=num_class)\n",
        "\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        self._create_weights(mean,std) # weight initialization\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        \"\"\"\n",
        "        Forward pass through CNN\n",
        "        @params inputs (torch.Tensor): Tensor of shape \n",
        "        (batch_size,num_characters,max_seq_len) representing a batch of\n",
        "        sentences\n",
        "        @returns x (torch.Tensor): Tensor of shape (batch_size,num_cls) that\n",
        "        contains a batch of vectors having unnormalized log probabilities for\n",
        "        each class computed according to the input text\n",
        "        \"\"\"\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.log_softmax(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def _create_weights(self, mean, std):\n",
        "        \"\"\"\n",
        "        Initialization of weights using a Gaussian distribution\n",
        "        @params mean (int): Mean of the distribution\n",
        "        @params std (int): Standard deviation of the distribution\n",
        "        \"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
        "                module.weight.data.normal_(mean, std)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TotHFalQ1inW",
        "colab_type": "text"
      },
      "source": [
        "## **Weights & Biases for visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb6g5E_T5SCE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a9d94dc-8717-4dbc-e2f0-e040e85b0f80"
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 26fd22ecbe5d0d2e53f656dbee7cedad16503a06"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/8c/683d6112dfeaca2c6bc47ef6b4a92b60a310e5054df90bd8d920d6c7a275/wandb-0.9.3-py2.py3-none-any.whl (1.4MB)\n",
            "\r\u001b[K     |▎                               | 10kB 25.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 29.3MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 32.5MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 23.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 15.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 12.4MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 12.2MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 11.4MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 10.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 10.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 10.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 10.7MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 10.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 143kB 10.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 153kB 10.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 163kB 10.7MB/s eta 0:00:01\r\u001b[K     |████                            | 174kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 184kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 194kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 204kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 215kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 235kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 245kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 256kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 266kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 276kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 286kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 296kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 307kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 317kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 327kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 337kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 348kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 368kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 378kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 389kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 399kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 409kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 419kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 430kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 440kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 450kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 460kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 471kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 481kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 491kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 501kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 512kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 522kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 532kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 542kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 552kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 563kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 573kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 583kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 593kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 604kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 614kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 624kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 634kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 645kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 655kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 665kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 675kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 686kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 696kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 706kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 716kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 727kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 737kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 747kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 757kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 768kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 778kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 788kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 798kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 808kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 819kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 829kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 839kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 849kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 860kB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 870kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 880kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 890kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 901kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 911kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 921kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 931kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 942kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 952kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 962kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 972kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 983kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 993kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.0MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.2MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.3MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/52/e2604ee8f48219ddb177fffd222f9ed54e684621922ed2f9be85a7481929/sentry_sdk-0.16.1-py2.py3-none-any.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 55.3MB/s \n",
            "\u001b[?25hCollecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz (94kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.3MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/1e/a45320cab182bf1c8656107b3d4c042e659742822fc6bff150d769a984dd/GitPython-3.1.7-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 56.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.8MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: watchdog, subprocess32, gql, pathtools, graphql-core\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73870 sha256=97bf0f7280c92471e64aaf16a5cea8a47ddeffaa006f1bd85d83b9de81b4a51b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=7de0719411816d6b866d84ade68851e4d2189f777e346f88f270c27d6767256c\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=8c408c0524430ba17169e9ec228ddf8e435befc29fda0bc63a4c257ca32885a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=cf6490ab4278f1f40d091ef73755fb32e12c19eb9db815a5e4b5f0cc706b80f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=f3bc22afc4f37d95308b292456ae5e066d7c8aaeeb3a8ae27b34081b907b41fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "Successfully built watchdog subprocess32 gql pathtools graphql-core\n",
            "Installing collected packages: configparser, sentry-sdk, pathtools, watchdog, subprocess32, shortuuid, docker-pycreds, graphql-core, gql, smmap, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.7 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.16.1 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.9.3 watchdog-0.10.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDE8gujF1e90",
        "colab_type": "text"
      },
      "source": [
        "## **Train - Eval loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gcvmLl4jCGh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "38720228-e41e-4196-d44a-6da57b892d7e"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import wandb\n",
        "from sklearn import metrics\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    \"\"\"\n",
        "    Gets the current learning rate from the optimizer during training\n",
        "    @params optimizer (torch.optim.Optimizer): Optimizer object\n",
        "    @returns param_group['lr'] (int): Current learning rate\n",
        "    \"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "wandb.init(project=\"charcnn\")\n",
        "\n",
        "model_size = 'large'\n",
        "\n",
        "dataset = 'sogou_news'\n",
        "\n",
        "datasets = ['ag_news',\n",
        "           'sogou_news',\n",
        "           'dbpedia',\n",
        "           'yelp_review_full',\n",
        "           'yelp_review_polarity',\n",
        "           'amazon_review_full',\n",
        "           'amazon_review_polarity',\n",
        "           'yahoo_answers']\n",
        "\n",
        "model_params = {\n",
        "    'small':{'fc_size':1024,'conv_channel_size':256,'mean':0,'std':0.05},\n",
        "    'large':{'fc_size':2048,'conv_channel_size':1024,'mean':0,'std':0.02}\n",
        "    }\n",
        "\n",
        "dataset_info = {'ag_news':\n",
        "         {'train_path':'./ag_news_csv/train.csv',\n",
        "          'test_path':'./ag_news_csv/test.csv',\n",
        "          'num_classes':4},\n",
        "         'sogou_news':\n",
        "         {'train_path':'./sogou_news_csv/train.csv',\n",
        "          'test_path':'./sogou_news_csv/test.csv',\n",
        "          'num_classes':5},\n",
        "         'dbpedia':\n",
        "         {'train_path':'./dbpedia_csv/train.csv',\n",
        "          'test_path':'./dbpedia/test.csv',\n",
        "          'num_classes':14},\n",
        "         'yelp_review_full':\n",
        "         {'train_path':'./yelp_review_full_csv/train.csv',\n",
        "          'test_path':'./yelp_review_full_csv/test.csv',\n",
        "          'num_classes':5},\n",
        "         'yelp_review_polarity':\n",
        "         {'train_path':'./yelp_review_polarity_csv/train.csv',\n",
        "          'test_path':'./yelp_review_polarity_csv/test.csv',\n",
        "          'num_classes':2},\n",
        "         'amazon_review_full':\n",
        "         {'train_path':'./amazon_review_full_csv/train.csv',\n",
        "          'test_path':'./amazon_review_full_csv/test.csv',\n",
        "          'num_classes':5},\n",
        "         'amazon_review_polarity':\n",
        "         {'train_path':'./amazon_review_polarity_csv/train.csv',\n",
        "          'test_path':'./amazon_review_polarity_csv/test.csv',\n",
        "          'num_classes':2},\n",
        "         'yahoo_answers':\n",
        "         {'train_path':'./yahoo_answers_csv/train.csv',\n",
        "          'test_path':'./yahoo_answers_csv/test.csv',\n",
        "          'num_classes':10}\n",
        "         }\n",
        "\n",
        "train_path = dataset_info[dataset]['train_path']\n",
        "test_path = dataset_info[dataset]['test_path']\n",
        "alpha_path = 'alphabet.json' \n",
        "max_seq_len = 1014\n",
        "batch_size = 128\n",
        "num_classes = dataset_info[dataset]['num_classes']\n",
        "fc_size = model_params[model_size]['fc_size']\n",
        "conv_channel_size = model_params[model_size]['conv_channel_size']\n",
        "num_characters = 70\n",
        "milestones = [3,6,9,12,15,18,21,24,27,30]\n",
        "total_epochs = 200\n",
        "resume = 1\n",
        "model_path = os.path.join('./models',dataset+'_'+model_size)\n",
        "model_name = 'bestmodel.ckpt'\n",
        "start_epoch = 1\n",
        "save_every = 4\n",
        "print_every = 1\n",
        "loss_history = []\n",
        "max_norm = 400\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ag_dataset_train = AGNEWS_Dataset(train_path,alpha_path,max_seq_len)\n",
        "dataloader_train = DataLoader(ag_dataset_train,batch_size=128,shuffle=True,num_workers=4)\n",
        "ag_dataset_test = AGNEWS_Dataset(test_path,alpha_path,max_seq_len)\n",
        "dataloader_test = DataLoader(ag_dataset_test,batch_size=128,shuffle=True,num_workers=4)\n",
        "\n",
        "\n",
        "model = charCNN(num_characters,conv_channel_size,fc_size,num_classes,max_seq_len,model_params[model_size]['mean'],model_params[model_size]['std'])\n",
        "wandb.watch(model)\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "if resume:\n",
        "    checkpoint = torch.load(os.path.join(model_path,model_name))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    #scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    loss_history = checkpoint['loss_history']\n",
        "    print('Loaded model from checkpoint ...')\n",
        "\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.5, last_epoch=-1)\n",
        "\n",
        "acc_history = [0]\n",
        "\n",
        "for epoch in range(start_epoch,total_epochs+1):\n",
        "    batch_loss_history = []\n",
        "    for i,(char_seq,cls) in enumerate(dataloader_train):\n",
        "\n",
        "        cls = torch.LongTensor(cls)\n",
        "        \n",
        "        char_seq = char_seq.to(device)\n",
        "        cls = cls.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(char_seq)\n",
        "        loss = F.nll_loss(outputs,cls-1)\n",
        "        batch_loss_history.append(loss.item())\n",
        "        loss_history.append(loss.item())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "    acc = 0\n",
        "    batches = 0\n",
        "    for i,(char_seq,cls) in enumerate(dataloader_test):\n",
        "        batches += 1\n",
        "        cls = torch.LongTensor(cls)\n",
        "        \n",
        "        char_seq = char_seq.to(device)\n",
        "        cls = cls.to(device)\n",
        "        \n",
        "        outputs = model(char_seq)\n",
        "        predicted_cls = outputs.max(1)[1]\n",
        "        acc += metrics.accuracy_score((cls-1).cpu().numpy(),predicted_cls.cpu().numpy())\n",
        "    avg_accuracy = acc/batches\n",
        "    \n",
        "\n",
        "    if epoch == start_epoch or avg_accuracy > max(acc_history):\n",
        "        torch.save(\n",
        "            {\n",
        "                'model_state_dict':model.state_dict(),\n",
        "                'optimizer_state_dict':optimizer.state_dict(),\n",
        "                'scheduler_state_dict':scheduler.state_dict(),\n",
        "                'epoch':epoch+1,\n",
        "                'loss_history':loss_history\n",
        "            },\n",
        "            os.path.join(model_path,'bestmodel.ckpt')\n",
        "            )\n",
        "        print('Saved model ...')\n",
        "\n",
        "    acc_history.append(avg_accuracy)\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        mean_loss_per_epoch = sum(batch_loss_history)/len(batch_loss_history)\n",
        "        print('[{}/{}] Loss: {}'.format(epoch,total_epochs,mean_loss_per_epoch))\n",
        "        wandb.log({\"Train Loss\": mean_loss_per_epoch,\"Learning Rate\": get_lr(optimizer)})\n",
        "        print('Accruacy: ',avg_accuracy)\n",
        "        print('Test error: ',1-avg_accuracy)\n",
        "        wandb.log({\"Test Accuracy\": avg_accuracy,\"Test Error\": 1-avg_accuracy})\n",
        "\n",
        "\n",
        "\n",
        "print('Training complete')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/sridhar98/charcnn\" target=\"_blank\">https://app.wandb.ai/sridhar98/charcnn</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/sridhar98/charcnn/runs/11urxcy3\" target=\"_blank\">https://app.wandb.ai/sridhar98/charcnn/runs/11urxcy3</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded model from checkpoint ...\n",
            "Saved model ...\n",
            "[6/200] Loss: 0.12002248339677088\n",
            "Accruacy:  0.9449904495380241\n",
            "Test error:  0.055009550461975865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hbFfw81ISd",
        "colab_type": "text"
      },
      "source": [
        "## **Accruacy:  0.8794270833333333**\n",
        "## **Test error:  0.1205729166666667**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keZhKOtK1axB",
        "colab_type": "text"
      },
      "source": [
        "## **Find best model** (not used)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwBtB0uE5Uk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "alpha_path = 'alphabet.json' \n",
        "max_seq_len = 1014\n",
        "batch_size = 128\n",
        "num_classes = 4\n",
        "fc_size = 1024\n",
        "conv_channel_size = 256\n",
        "num_characters = 70\n",
        "model_path = './models/'\n",
        "model_name = ['model_24_.ckpt','model_16_.ckpt','model_12_.ckpt','model_8_.ckpt','model_4_.ckpt']\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ag_dataset_test = AGNEWS_Dataset(test_path,alpha_path,max_seq_len)\n",
        "dataloader_test = DataLoader(ag_dataset_test,batch_size=128,shuffle=True,num_workers=4)\n",
        "\n",
        "model = charCNN(num_characters,conv_channel_size,fc_size,num_classes)\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for mname in model_name:\n",
        "\n",
        "    checkpoint = torch.load(os.path.join(model_path,mname))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print('Loaded model from checkpoint ...')\n",
        "\n",
        "    acc = 0\n",
        "    batches = 0\n",
        "    for i,(char_seq,cls) in enumerate(dataloader_test):\n",
        "        batches += 1\n",
        "        cls = torch.LongTensor(cls)\n",
        "        \n",
        "        char_seq = char_seq.to(device)\n",
        "        cls = cls.to(device)\n",
        "        \n",
        "        outputs = model(char_seq)\n",
        "        predicted_cls = outputs.max(1)[1]\n",
        "        acc += metrics.accuracy_score((cls-1).cpu().numpy(),predicted_cls.cpu().numpy())\n",
        "    avg_accuracy = acc/batches\n",
        "    print('Model: ',mname)\n",
        "    print('Accruacy: ',avg_accuracy)\n",
        "    print('Test error: ',1-avg_accuracy)\n",
        "    print('-----------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}