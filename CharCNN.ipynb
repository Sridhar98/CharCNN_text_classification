{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CharCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx6hQlsE12b6",
        "colab_type": "text"
      },
      "source": [
        "## **Mount Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeaQAPO-s-az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxpSsgSt1wxA",
        "colab_type": "text"
      },
      "source": [
        "## **Change directory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-ohhxRAtRnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /gdrive/My Drive/IIITH/CharCNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZJ-Jdqb1tDJ",
        "colab_type": "text"
      },
      "source": [
        "## **Data Loader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02vgB8NbtXjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import csv\n",
        "import json\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class AGNEWS_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    Defines the AG News dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,csv_path,alphabet_path,max_seq_len):\n",
        "        \"\"\"\n",
        "        Initializes the dataset\n",
        "        @params csv_path (str): Path to csv file that contains train/test data\n",
        "        @params alphabet_path (str): Path to json file that contains the\n",
        "        characters considered\n",
        "        @params max_seq_len (int): Maximum number of characters considered]\n",
        "        for input\n",
        "        \"\"\"\n",
        "        self.max_seq_len = max_seq_len\n",
        "        with open(alphabet_path) as f:\n",
        "            self.alphabet = json.load(f)\n",
        "        with open(csv_path) as f:\n",
        "            self.data = csv.reader(f,delimiter=',')\n",
        "            self.data = list(self.data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        \"\"\"\n",
        "        Returns a text, class pair\n",
        "        @params idx (int): Index into the dataset\n",
        "        @returns (self.seq,self.cls) tuple(torch.Tensor,int): Returns a tensor\n",
        "        of shape (num_characters,max_seq_len) representing the input text and an\n",
        "        integer representing the class index\n",
        "        \"\"\"\n",
        "        self.cls = int(self.data[idx][0])\n",
        "        self.seq = torch.zeros(len(self.alphabet),self.max_seq_len)\n",
        "        seq_len = 0\n",
        "        sequence = \"\".join(self.data[idx][1:])\n",
        "        sequence = sequence[::-1]\n",
        "        for char in sequence:\n",
        "            if seq_len > self.max_seq_len:\n",
        "                break\n",
        "            try:\n",
        "                self.seq[self.alphabet.index(char)][seq_len] = 1\n",
        "            except:\n",
        "                pass\n",
        "            seq_len += 1\n",
        "        return self.seq,self.cls\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the dataset length\n",
        "        \"\"\"\n",
        "        return len(list(self.data))\n",
        "\n",
        "def one_hot(data,alphabet):\n",
        "    \"\"\"\n",
        "    Converts a character to its one-hot vector representation\n",
        "    @params data (char): The character that is input to the CNN\n",
        "    @params alphabet (list): The list of characters considered.\n",
        "    NOTE: Characters outside the alphabet are considered to be a zero vector\n",
        "    @returns t (torch.Tensor): Tensor of shape (len(alphabet))\n",
        "    \"\"\"\n",
        "    t = torch.zeros(len(alphabet))\n",
        "    try:\n",
        "        t[alphabet.index(data)] = 1\n",
        "    except:\n",
        "        return t\n",
        "    return t\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrWHrwLq1m_Y",
        "colab_type": "text"
      },
      "source": [
        "## **Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDPJUfZ7Dc_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class charCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines the character level CNN architecture\n",
        "    \"\"\"\n",
        "    def __init__(self,num_features,conv_channel_size,fc_size,num_class):\n",
        "        \"\"\"\n",
        "        Initializes the model\n",
        "        @params num_features (int): The number of features (the number of\n",
        "         characters) considered\n",
        "        @params conv_channel_size (int): Number of 1D Convolutional kernels used\n",
        "        @params fc_size (int): Number of units in the fully-connected layers\n",
        "        @num_class (int): Number of classes in the dataset\n",
        "        @returns object of this class when implicitly called\n",
        "        \"\"\"\n",
        "        super(charCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=num_features,out_channels=conv_channel_size,kernel_size=7,stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3,stride=3)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=conv_channel_size,out_channels=conv_channel_size,kernel_size=7,stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3,stride=3)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=conv_channel_size,out_channels=conv_channel_size,kernel_size=3,stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=conv_channel_size,out_channels=conv_channel_size,kernel_size=3,stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=conv_channel_size,out_channels=conv_channel_size,kernel_size=3,stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=conv_channel_size,out_channels=conv_channel_size,kernel_size=3,stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3,stride=3)\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(in_features=8704,out_features=fc_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(in_features=fc_size,out_features=fc_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.fc3 = nn.Linear(in_features=fc_size,out_features=num_class)\n",
        "\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        self._create_weights() # weight initialization\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        \"\"\"\n",
        "        Forward pass through CNN\n",
        "        @params inputs (torch.Tensor): Tensor of shape \n",
        "        (batch_size,num_characters,max_seq_len) representing a batch of\n",
        "        sentences\n",
        "        @returns x (torch.Tensor): Tensor of shape (batch_size,num_cls) that\n",
        "        contains a batch of vectors having unnormalized log probabilities for\n",
        "        each class computed according to the input text\n",
        "        \"\"\"\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.log_softmax(x)\n",
        "        return x\n",
        "\n",
        "    def _create_weights(self, mean=0.0, std=0.05):\n",
        "        \"\"\"\n",
        "        Initialization of weights using a Gaussian distribution\n",
        "        @params mean (int): Mean of the distribution\n",
        "        @params std (int): Standard deviation of the distribution\n",
        "        \"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
        "                module.weight.data.normal_(mean, std)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TotHFalQ1inW",
        "colab_type": "text"
      },
      "source": [
        "## **Weights & Biases for visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb6g5E_T5SCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade wandb\n",
        "!wandb login 26fd22ecbe5d0d2e53f656dbee7cedad16503a06"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDE8gujF1e90",
        "colab_type": "text"
      },
      "source": [
        "## **Train - Eval loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gcvmLl4jCGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    \"\"\"\n",
        "    Gets the current learning rate from the optimizer during training\n",
        "    @params optimizer (torch.optim.Optimizer): Optimizer object\n",
        "    @returns param_group['lr'] (int): Current learning rate\n",
        "    \"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "wandb.init(project=\"charcnn\")\n",
        "\n",
        "train_path = './ag_news_csv/train.csv'\n",
        "test_path = './ag_news_csv/test.csv'\n",
        "alpha_path = 'alphabet.json' \n",
        "max_seq_len = 1014\n",
        "batch_size = 128\n",
        "num_classes = 4\n",
        "fc_size = 1024\n",
        "conv_channel_size = 256\n",
        "num_characters = 70\n",
        "milestones = [3,6,9,12,15,18,21,24,27,30]\n",
        "total_epochs = 200\n",
        "resume = 0\n",
        "model_path = './models/'\n",
        "model_name = 'model_22_.ckpt'\n",
        "start_epoch = 1\n",
        "save_every = 4\n",
        "print_every = 1\n",
        "loss_history = []\n",
        "max_norm = 400\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ag_dataset_train = AGNEWS_Dataset(train_path,alpha_path,max_seq_len)\n",
        "dataloader_train = DataLoader(ag_dataset_train,batch_size=128,shuffle=True,num_workers=4)\n",
        "ag_dataset_test = AGNEWS_Dataset(test_path,alpha_path,max_seq_len)\n",
        "dataloader_test = DataLoader(ag_dataset_test,batch_size=128,shuffle=True,num_workers=4)\n",
        "\n",
        "\n",
        "model = charCNN(num_characters,conv_channel_size,fc_size,num_classes)\n",
        "wandb.watch(model)\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "if resume:\n",
        "    checkpoint = torch.load(os.path.join(model_path,model_name))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    #scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    loss_history = checkpoint['loss']\n",
        "    print('Loaded model from checkpoint ...')\n",
        "\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.5, last_epoch=-1)\n",
        "\n",
        "acc_history = [0]\n",
        "\n",
        "for epoch in range(start_epoch,total_epochs+1):\n",
        "    batch_loss_history = []\n",
        "    for i,(char_seq,cls) in enumerate(dataloader_train):\n",
        "\n",
        "        cls = torch.LongTensor(cls)\n",
        "        \n",
        "        char_seq = char_seq.to(device)\n",
        "        cls = cls.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(char_seq)\n",
        "        loss = F.nll_loss(outputs,cls-1)\n",
        "        batch_loss_history.append(loss.item())\n",
        "        loss_history.append(loss.item())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "    acc = 0\n",
        "    batches = 0\n",
        "    for i,(char_seq,cls) in enumerate(dataloader_test):\n",
        "        batches += 1\n",
        "        cls = torch.LongTensor(cls)\n",
        "        \n",
        "        char_seq = char_seq.to(device)\n",
        "        cls = cls.to(device)\n",
        "        \n",
        "        outputs = model(char_seq)\n",
        "        predicted_cls = outputs.max(1)[1]\n",
        "        acc += metrics.accuracy_score((cls-1).cpu().numpy(),predicted_cls.cpu().numpy())\n",
        "    avg_accuracy = acc/batches\n",
        "    \n",
        "\n",
        "    if epoch == start_epoch or avg_accuracy > max(acc_history):\n",
        "        torch.save(\n",
        "            {\n",
        "                'model_state_dict':model.state_dict(),\n",
        "                'optimizer_state_dict':optimizer.state_dict(),\n",
        "                'scheduler_state_dict':scheduler.state_dict(),\n",
        "                'epoch':epoch+1,\n",
        "                'loss_history':loss_history\n",
        "            },\n",
        "            os.path.join(model_path,'model_'+str(epoch)+'_.ckpt')\n",
        "            )\n",
        "        print('Saved model ...')\n",
        "\n",
        "    acc_history.append(avg_accuracy)\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        mean_loss_per_epoch = sum(batch_loss_history)/len(batch_loss_history)\n",
        "        print('[{}/{}] Loss: {}'.format(epoch,total_epochs,mean_loss_per_epoch))\n",
        "        wandb.log({\"Train Loss\": mean_loss_per_epoch,\"Learning Rate\": get_lr(optimizer)})\n",
        "        print('Accruacy: ',avg_accuracy)\n",
        "        print('Test error: ',1-avg_accuracy)\n",
        "        wandb.log({\"Test Accuracy\": avg_accuracy,\"Test Error\": 1-avg_accuracy})\n",
        "\n",
        "\n",
        "\n",
        "print('Training complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hbFfw81ISd",
        "colab_type": "text"
      },
      "source": [
        "## **Accruacy:  0.8794270833333333**\n",
        "## **Test error:  0.1205729166666667**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keZhKOtK1axB",
        "colab_type": "text"
      },
      "source": [
        "## **Find best model** (not used)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwBtB0uE5Uk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "alpha_path = 'alphabet.json' \n",
        "max_seq_len = 1014\n",
        "batch_size = 128\n",
        "num_classes = 4\n",
        "fc_size = 1024\n",
        "conv_channel_size = 256\n",
        "num_characters = 70\n",
        "model_path = './models/'\n",
        "model_name = ['model_24_.ckpt','model_16_.ckpt','model_12_.ckpt','model_8_.ckpt','model_4_.ckpt']\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ag_dataset_test = AGNEWS_Dataset(test_path,alpha_path,max_seq_len)\n",
        "dataloader_test = DataLoader(ag_dataset_test,batch_size=128,shuffle=True,num_workers=4)\n",
        "\n",
        "model = charCNN(num_characters,conv_channel_size,fc_size,num_classes)\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for mname in model_name:\n",
        "\n",
        "    checkpoint = torch.load(os.path.join(model_path,mname))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print('Loaded model from checkpoint ...')\n",
        "\n",
        "    acc = 0\n",
        "    batches = 0\n",
        "    for i,(char_seq,cls) in enumerate(dataloader_test):\n",
        "        batches += 1\n",
        "        cls = torch.LongTensor(cls)\n",
        "        \n",
        "        char_seq = char_seq.to(device)\n",
        "        cls = cls.to(device)\n",
        "        \n",
        "        outputs = model(char_seq)\n",
        "        predicted_cls = outputs.max(1)[1]\n",
        "        acc += metrics.accuracy_score((cls-1).cpu().numpy(),predicted_cls.cpu().numpy())\n",
        "    avg_accuracy = acc/batches\n",
        "    print('Model: ',mname)\n",
        "    print('Accruacy: ',avg_accuracy)\n",
        "    print('Test error: ',1-avg_accuracy)\n",
        "    print('-----------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}